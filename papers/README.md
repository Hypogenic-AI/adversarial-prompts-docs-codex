# Downloaded Papers

1. [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](2511.10720_PISanitizer_long_context_prompt_sanitization.pdf)
   - Authors: Runpeng Geng, Yanting Wang, Chenlong Yin, Minhao Cheng, Ying Chen, Jinyuan Jia
   - Year: 2025
   - arXiv: 2511.10720
   - Why relevant: Focuses on prompt injection in long-context settings and proposes a defense tailored to long documents.

2. [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](2512.10449_indirect_prompt_injection_scientific_reviewers.pdf)
   - Authors: Devanshu Sahoo, Manish Prasad, Vasudev Majhi, Jahnvi Singh, Vinay Chamola, Yash Sinha, et al.
   - Year: 2025
   - arXiv: 2512.10449
   - Why relevant: Studies indirect prompt injection embedded in long-form scientific papers and measures impact on reviewer decisions.

3. [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](2512.23684_multilingual_hidden_prompt_injection_academic_review.pdf)
   - Authors: Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai.-Doss
   - Year: 2025
   - arXiv: 2512.23684
   - Why relevant: Investigates hidden prompt injection inside full academic papers, including multilingual concealment.

4. [ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data](2512.09321_ObliInjection_order_oblivious_prompt_injection.pdf)
   - Authors: Reachal Wang, Yuqi Jia, Neil Zhenqiang Gong
   - Year: 2025
   - arXiv: 2512.09321
   - Why relevant: Addresses attack success when malicious instructions are hidden across multiple sources and re-ordered, similar to long document contexts.

5. [BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents](2511.20597_BrowseSafe_prompt_injection_browser_agents.pdf)
   - Authors: Kaiyuan Zhang, Mark Tenenholtz, Kyle Polley, Jerry Ma, Denis Yarats, Ninghui Li
   - Year: 2025
   - arXiv: 2511.20597
   - Why relevant: Provides a benchmark of prompt injection attacks embedded in realistic HTML pages, relevant to hiding instructions in long documents.

6. [Securing Large Language Models (LLMs) from Prompt Injection Attacks](2512.01326_securing_llms_from_prompt_injection.pdf)
   - Authors: Omar Farooq Khan Suri, John McCrae
   - Year: 2025
   - arXiv: 2512.01326
   - Why relevant: Evaluates defenses against prompt injection, offering baselines for experiments.
